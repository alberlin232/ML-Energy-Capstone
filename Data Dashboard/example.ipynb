{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_day = 0\n",
    "start_month = 0\n",
    "end_day = 0\n",
    "end_month = 0\n",
    "location = \"\"\n",
    "locations = ['Aguayo', 'Ajax', 'Amazon', 'Anacacho', 'Appaloosa Run', 'Aquilla Lake 2', 'Aquilla Lake 1', 'Armstrong',\n",
    "             'Azure Sky 1', 'Azure Sky 2', 'Baffin', 'Baird North', 'Barranca 2', 'Barranca 1', 'Barrow Ranch',\n",
    "             'Barton Chapel', 'Bearkat', 'Bethel', 'Big Sampson', 'Big Spring', 'Black Jack Creek', 'Blanco Canyon',\n",
    "             'Blue Summit 2', 'Blue Summit Repower', 'Blue Summit 1', 'Blue Summit 3', 'Bobcate Bluff Repower',\n",
    "             'Bobcat Bluff', 'Brazos', 'Briscoe', 'Broadview Energy', 'Bruennings Breeze', 'Buckthorn',\n",
    "             'Buenos Aires 1', 'Buenos Aires 2', 'Bufallo Gap 4', 'Buffalo Gap', 'Cabezon', 'Cactus Flats',\n",
    "             'Callahan Divide', 'Cameron', 'Camp Springs 2', 'Canadian Breaks', 'Canyon Wind', 'Capricorn 3',\n",
    "             'Capricorn 4', 'Capricorn Ridge', 'Cattleman Wind A', 'Cattleman Wind B']\n",
    "def update_start_day(limit):\n",
    "    \n",
    "    start_day = limit\n",
    "    \n",
    "    #print(\"Start Day: \"+str(start_day))\n",
    "\n",
    "def update_end_day(limit):\n",
    "    \n",
    "    end_day = limit\n",
    "    \n",
    "    #print(\"End Day: \"+str(end_day))\n",
    "\n",
    "def update_start_month(limit):\n",
    "    \n",
    "    start_month = limit\n",
    "    \n",
    "    #print(\"Start Day: \"+str(start_month))\n",
    "\n",
    "def update_end_month(limit):\n",
    "    \n",
    "    end_month = limit\n",
    "    \n",
    "    #print(\"Start Day: \"+str(end_month))\n",
    "\n",
    "def update_location(limit):\n",
    "    \n",
    "    location = limit\n",
    "    \n",
    "    #print(\"Location: \"+str(location))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "style = {'description_width': 'initial'}\n",
    "days = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]\n",
    "months = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "location = widgets.Dropdown(\n",
    "    options=locations,\n",
    "    value='Aguayo',\n",
    "    description='Wind Locations',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "start_m = widgets.Dropdown(\n",
    "    options=months,\n",
    "    value=6,\n",
    "    description='Start Month',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "start_d = widgets.Dropdown(\n",
    "    options=days,\n",
    "    value=15,\n",
    "    description='Start Day',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "\n",
    "end_m = widgets.Dropdown(\n",
    "    options=months,\n",
    "    value=6,\n",
    "    description='End Month',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "end_d = widgets.Dropdown(\n",
    "    options=days,\n",
    "    value=16,\n",
    "    description='End Day',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "path = \"Data/Wind/\"\n",
    "files = glob.glob(path + \"*.h5\")\n",
    "files.sort()\n",
    "#print(files)\n",
    "#s_path = \"Data/Solar/\"\n",
    "#s_files = glob.glob(s_path + \"*.h5\")\n",
    "#s_files.sort()\n",
    "#print(s_files)\n",
    "\n",
    "\n",
    "\n",
    "def update_graph(start_day,start_month,end_day,end_month,location):\n",
    "    for i,loc in enumerate(locations):\n",
    "        if loc == location:\n",
    "            index = i\n",
    "            break\n",
    "    with h5py.File(files[index], 'r') as f:\n",
    "        print(f)\n",
    "        actuals = pd.DataFrame(f['actuals'][...])\n",
    "        actual_time_steps = pd.to_datetime(f['time_index'][...].astype(str), errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    with h5py.File(files[index+264], 'r') as f:\n",
    "        #print(f)\n",
    "        forecasts= pd.DataFrame(f['forecasts'][...])\n",
    "        time_intervals = pd.DataFrame(f['forecast_time'][...]) \n",
    "        time_intervals[0] = time_intervals[0].str.decode(\"utf-8\") #Convert from byte array to string\n",
    "        fcst_time_steps = pd.to_datetime(f['forecast_time'][...].astype(str), format='%Y-%m-%d %H:%M:%S%z', errors='raise', utc = True)\n",
    "        percentile50 = pd.DataFrame(forecasts[49]); #50th PERCENTILE\n",
    "        percentile25 = pd.DataFrame(forecasts[24]); #50th PERCENTILE\n",
    "        percentile75 = pd.DataFrame(forecasts[74]); #50th PERCENTILE\n",
    "        percentile5 = pd.DataFrame(forecasts[4]); #50th PERCENTILE\n",
    "        percentile95 = pd.DataFrame(forecasts[94]); #50th PERCENTILE\n",
    "\n",
    "    plt.plot(actual_time_steps, actuals[0], label = \"Actuals\", linewidth = 3.5)\n",
    "    plt.plot(fcst_time_steps, percentile50, \"r-\", label= \"50th Percentile\")\n",
    "    plt.plot(fcst_time_steps, percentile25, \"g-\", label= \"25th Percentile\")\n",
    "    plt.plot(fcst_time_steps, percentile75, color = \"orange\", label= \"75th Percentile\")\n",
    "    plt.plot(fcst_time_steps, percentile95, color = \"purple\", label= \"95th Percentile\")\n",
    "    plt.plot(fcst_time_steps, percentile5, color = \"pink\", label= \"5th Percentile\")\n",
    "    plt.xlim(datetime.datetime(2018,int(start_month),int(start_day)), datetime.datetime(2018,int(end_month),int(end_day))) # Format:(year, month, day, hour, min, sec)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.margins(0.2)\n",
    "    plt.xlabel(\"Date/Hour\")\n",
    "    plt.ylabel(\"Power [MW]\")\n",
    "    plt.title(\"Wind Actual vs Day-Ahead Forecast\")\n",
    "    plt.legend()\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import h5py\n",
    "#import pandas as pd\n",
    "# import s3fs\n",
    "#import matplotlib.pyplot as plt\n",
    "#import datetime\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pathlib import Path\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pandas.plotting import autocorrelation_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_day1 = 0\n",
    "end_day1 = 0\n",
    "length = 0\n",
    "location1 = \"\"\n",
    "locations2 = ['Data/Load/Zone_Coast_load_actuals_2018.h5', 'Data/Load/Zone_East_load_actuals_2018.h5', 'Data/Load/Zone_Far_West_load_actuals_2018.h5',\n",
    "             'Data/Load/Zone_West_load_actuals_2018.h5', 'Data/Load/Zone_North_load_actuals_2018.h5','Data/Load/Zone_North_Central_load_actuals_2018.h5',\n",
    "             'Data/Load/Zone_Southern_load_actuals_2018.h5', 'Data/Load/Zone_South_Central_load_actuals_2018.h5']\n",
    "locations1 = ['Coast Zone', 'East Zone', 'Far West Zone','West Zone', 'North Zone', 'North Central Zone', 'Southern Zone', 'South Central Zone']\n",
    "def update_start_day1(limit):\n",
    "    \n",
    "    start_day1 = limit\n",
    "    \n",
    "    #print(\"Start Day: \"+str(start_day))\n",
    "\n",
    "\n",
    "def update_location1(limit):\n",
    "    \n",
    "    location1 = limit\n",
    "    \n",
    "    #print(\"Location: \"+str(location))\n",
    "\n",
    "def update_length(limit):\n",
    "    length = limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "days1 = list(range(1,366))\n",
    "\n",
    "location1 = widgets.Dropdown(\n",
    "    options=locations1,\n",
    "    value='Coast Zone',\n",
    "    description='Load Locations',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "\n",
    "start_d1 = widgets.Dropdown(\n",
    "    options=days1,\n",
    "    value=15,\n",
    "    description='Start Day',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "\n",
    "length1 = widgets.Dropdown(\n",
    "    options=days1,\n",
    "    value=12,\n",
    "    description='Number of 5 min intervals to predict',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path1 = \"Data/Load/\"\n",
    "files1 = glob.glob(path1 + \"*.h5\")\n",
    "files1.sort()\n",
    "\n",
    "def update_graph1(start_day1,location1, length):\n",
    "    index1 = 0\n",
    "    figure, axis = plt.subplots(1, 1)\n",
    "    for i,loc in enumerate(locations1):\n",
    "        if loc == location:\n",
    "            index1 = i\n",
    "            break\n",
    "    with h5py.File(files1[index1], 'r') as f:\n",
    "        print(f)\n",
    "        actuals = pd.DataFrame(f['actuals'][...])\n",
    "        # actual_time_steps = pd.to_datetime(f['time_index'][...].astype(str), errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "        actuals['date'] = pd.to_datetime(f['time_index'][...].astype(str), errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "        # dates = pd.DataFrame(f['time_index'][...].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        # df = pd.concat([actuals,actual_time_steps],axis = 1)\n",
    "        # autocorrelation_plot(actuals)\n",
    "        actuals.columns = ['Load','Date']\n",
    "        actuals = actuals.iloc[:,[1,0]]\n",
    "        # print(actuals.head())\n",
    "        # plt.plot(actuals['Date'], actuals['Load'])\n",
    "        # plt.show()\n",
    "        #autocorrelation_plot(actuals['Load'])\n",
    "    \n",
    "    daysinintervals = start_day1*288\n",
    "    train = actuals['Load'][(daysinintervals - 2016):daysinintervals] \n",
    "    test = actuals['Load'][daysinintervals:daysinintervals+length]\n",
    "    # print(test[52560], train)\n",
    "    history = [x for x in train]\n",
    "    expected = list()\n",
    "    predictions = list()\n",
    "    dates = list()\n",
    "    \n",
    "    for t in range(daysinintervals,daysinintervals+length):\n",
    "        model = ARIMA(history, order=(2,1,2))\n",
    "        # model = ARIMA(history, order=(1,1,1))\n",
    "        model_fit = model.fit()\n",
    "        output = model_fit.forecast()\n",
    "        # print(output)\n",
    "        yhat = output[0]\n",
    "        predictions.append(yhat)\n",
    "        obs = test[t]\n",
    "        date = actuals['Date'][t]\n",
    "        dates.append(date)\n",
    "        date = date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        history.append(test[t])\n",
    "        expected.append(test[t])\n",
    "        #print('%d Date = %s, Predicted = %f, Expected = %f' % ((t-daysinintervals),date, yhat, obs))\n",
    "    #evaluate forecasts\n",
    "    output = pd.DataFrame([dates, expected, predictions])\n",
    "    with open('ModelOutput.csv', 'w') as csv_file:\n",
    "        output.to_csv(csv_file, index = False)\n",
    "        \n",
    "    rmse = math.sqrt(sklearn.metrics.mean_squared_error(expected, predictions))\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    #plot forecasts against actual outcomes\n",
    "    plt.plot(dates,expected, label = \"actuals\")\n",
    "    plt.plot(dates,predictions, color = 'red', label = \"forecast\")\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.margins(0.2)\n",
    "    plt.xlabel(\"Date/Hour\")\n",
    "    plt.ylabel(\"Load [MWh]\")\n",
    "    plt.title(\"Actual vs ARIMA Forecast\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #autocorrelation_plot(actuals['Load'])\n",
    "    \n",
    "    #plot_pacf(train);\n",
    "    #plot_acf(train);\n",
    "    ad_fuller_result = adfuller(train)\n",
    "    #print(f'ADF Statistic: {ad_fuller_result[0]}')\n",
    "    #print(f'p-value: {ad_fuller_result[1]}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning model learning. This may take up to 90 seconds.\n",
      "MSE: 0.003, RMSE: 0.055\n"
     ]
    }
   ],
   "source": [
    "# pytorch mlp for regression\n",
    "from numpy import vstack\n",
    "from numpy import sqrt\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# Notes on allZonesErcot.csv\n",
    "# 0 - zone east\n",
    "# 1 - zone west\n",
    "# 2 - zone coast\n",
    " \n",
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path, header=None)\n",
    "        df = df.iloc[1: , :]  # skip 1st row\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, :-1].astype('float32')\n",
    "        self.y = df.values[:, -1].astype('float32')\n",
    "        # ensure target has the right shape\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    " \n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    " \n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    " \n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 10)\n",
    "        xavier_uniform_(self.hidden1.weight)\n",
    "        self.act1 = Sigmoid()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        xavier_uniform_(self.hidden2.weight)\n",
    "        self.act2 = Sigmoid()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(8, 1)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    " \n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        return X\n",
    " \n",
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    " \n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = MSELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(100):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    " \n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate mse\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    return mse\n",
    " \n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat\n",
    "\n",
    "def grade(yhat):\n",
    "    aScore = 0.063\n",
    "    res = ''\n",
    "    if yhat < aScore:\n",
    "        res = \"A\"\n",
    "    elif yhat < 2 * aScore:\n",
    "        res = \"B\"\n",
    "    elif yhat < 3 * aScore:\n",
    "        res = \"C\"\n",
    "    else:\n",
    "        res = \"D\"\n",
    "    print(\"Demand risk: \", res)\n",
    "        \n",
    " \n",
    "# prepare the data\n",
    "#path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "print('Machine learning model learning. This may take up to 90 seconds.')\n",
    "path = \"allZonesErcot.csv\"\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "#print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "# define the network\n",
    "model = MLP(3)\n",
    "# train the model\n",
    "train_model(train_dl, model)\n",
    "# evaluate the model\n",
    "mse = evaluate_model(test_dl, model)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (mse, sqrt(mse)))\n",
    "# make a prediction (expect class=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_day2 = 0\n",
    "\n",
    "def update_start_day2(limit):\n",
    "    \n",
    "    start_day2 = limit\n",
    "\n",
    "\n",
    "start_d2 = widgets.Dropdown(\n",
    "    options=days1,\n",
    "    value=15,\n",
    "    description='Start Day',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_graph2(start_day2):\n",
    "    rows = [[],[],[] ]\n",
    "    for h in range(3):  #zone code\n",
    "        for i in range(24):     # hour\n",
    "            rows[h].append([h, start_day2, i ] ) \n",
    "    #output\n",
    "    res = [0,0,0]\n",
    "    variations = [[],[],[]]\n",
    "    for h in range(3):  #zone code\n",
    "        for i in range(24):\n",
    "            yhat = predict(rows[h][i], model)\n",
    "            res[h] += yhat\n",
    "            variations[h].append(yhat[0])\n",
    "\n",
    "\n",
    "    #plot\n",
    "    x_axis = [i for i in range(24)]\n",
    "    y_axis_west = variations[0]\n",
    "    y_axis_east = variations[1]\n",
    "    y_axis_coast = variations[2]\n",
    "\n",
    "    plt.plot(x_axis, y_axis_west, label = 'Zone West', color = 'red')\n",
    "    plt.plot(x_axis, y_axis_east, label = 'Zone East', color = 'green')\n",
    "    plt.plot(x_axis, y_axis_coast, label = 'Zone Coast', color = 'blue')\n",
    "\n",
    "    plt.title('Day-Ahead-Market: Load variance on '+ str(1+start_day2//30) + \"/\" + str(start_day2%31) + \"/2022\")\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Variation')\n",
    "    res[0] = res[0]/24\n",
    "    res[1] = res[1]/24\n",
    "    res[2] = res[2]/24\n",
    "\n",
    "    print('\\nZone-West (red)')\n",
    "    print('Predicted Demand Variation: %.5f' % res[0])\n",
    "    grade(res[0])\n",
    "    print('\\nZone-East (green)')\n",
    "    print('Predicted Demand Variation: %.5f' % res[1])\n",
    "    grade(res[1])\n",
    "    print('\\nZone-Coast (blue)')\n",
    "    print('Predicted Demand Variation: %.5f' % res[2])\n",
    "    grade(res[2])\n",
    "\n",
    "    ymin = min( min(variations[0]), min(variations[1]), min(variations[2]) )\n",
    "    ymax = max( max(variations[0]) , max(variations[1]), max(variations[2]) )\n",
    "    plt.xlim([0, 24])\n",
    "    plt.ylim([ymin - 0.001, ymax + 0.001])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPA vs EIA datasets\n",
    "![alt text](boxplot.png)\n",
    "![alt text](hist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERCOT Generation Visualization\n",
    "![alt text](TAMU_Texas_generators.png)\n",
    "![alt text](hour.gif \"Renewable Energy Generation by Hour\")\n",
    "![alt text](month.gif \"Renewable Energy Generation by Month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERCOT Wind Generation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86f1caaad194e29ab2604e5acf69142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Start Day', index=14, options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widgets.interactive(update_graph, start_day=start_d, start_month=start_m, end_day=end_d, end_month=end_m, location=location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bca14401b24833aea7453cdd5fb7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Start Day', index=14, options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widgets.interactive(update_graph1, start_day1=start_d1, location1=location1, length=length1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Risk Rater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb05245c8694458489fc940fbf93e8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Start Day', index=26, options=(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widgets.interactive(update_graph2, start_day2=start_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
